{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import plac\n",
    "import argparse\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import compounding\n",
    "from spacy.util import minibatch\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "ner = pd.read_csv('data/ner.csv', encoding= 'unicode_escape',  error_bad_lines=False)\n",
    "df = pd.read_csv(\"data/ner_dataset.csv\", encoding= 'unicode_escape',  error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Sentence #', 'POS'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 2 columns):\n",
      "Word    1048575 non-null object\n",
      "Tag     1048575 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 16.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1048575</td>\n",
       "      <td>1048575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>35178</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>52573</td>\n",
       "      <td>887908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word      Tag\n",
       "count   1048575  1048575\n",
       "unique    35178       17\n",
       "top         the        O\n",
       "freq      52573   887908"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word Tag\n",
       "0      Thousands   O\n",
       "1             of   O\n",
       "2  demonstrators   O\n",
       "3           have   O\n",
       "4        marched   O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n",
       "       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n",
       "       'I-eve', 'I-nat'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data/ner_dataset_updated.csv\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting comma separated file to tab separated(TSV)\n",
    "# import csv\n",
    "\n",
    "# with open('data/ner_dataset_updated.csv','r') as csvin, open('data/ner_dataset_updated.tsv', 'w') as tsvout:\n",
    "#     csvin = csv.reader(csvin)\n",
    "#     tsvout = csv.writer(tsvout, delimiter='\\t')                        #  '\\t' tab separated\n",
    "\n",
    "#     for row in csvin:\n",
    "#         tsvout.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"data/ner_dataset_updated.tsv\", 'r')\n",
    "# for index, line in enumerate(f):\n",
    "#     print(line.split(\"\\t\"))\n",
    "#     break\n",
    "#     if index == 5:\n",
    "#         break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code of Blog\n",
    "\n",
    "def convert(output_path, unknown_label):\n",
    "    try:\n",
    "        fp = open(output_path, 'w')\n",
    "        s = ''\n",
    "        data_dict = {}\n",
    "        annotations = []\n",
    "        label_dict = {}\n",
    "        start = 0\n",
    "        for index, line in df.iterrows():\n",
    "            if line[0] != '.':\n",
    "                word = line[0]\n",
    "                entity = line[1]\n",
    "                s += word + \" \"\n",
    "                if entity != 'O':\n",
    "                    d = {}\n",
    "                    d['start'] = start\n",
    "                    d['end'] = d['start'] + len(word) - 1\n",
    "                    d['text'] = word\n",
    "                    try:\n",
    "                        label_dict[entity].append(d)\n",
    "                    except:\n",
    "                        label_dict[entity] = []\n",
    "                        label_dict[entity].append(d) \n",
    "                start += len(word) + 1\n",
    "            else:\n",
    "                data_dict['content'] = s\n",
    "                s = ''\n",
    "                label_list = []            \n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if label_dict[ents][i]['text'] != '':\n",
    "                            l = [ents, label_dict[ents][i]]\n",
    "                            for j in range(i+1, len(label_dict[ents])):\n",
    "                                if label_dict[ents][i]['text'] == label_dict[ents][j]['text']:\n",
    "                                    di = {}\n",
    "                                    di['start'] = label_dict[ents][j]['start']\n",
    "                                    di['end'] = label_dict[ents][j]['end']\n",
    "                                    di['text'] = label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text'] = ''\n",
    "                            label_list.append(l)  \n",
    "                for entities in label_list:\n",
    "                    label = {}\n",
    "                    label['label'] = [entities[0]]\n",
    "                    label['points'] = entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation'] = annotations\n",
    "                annotations = []\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict = {}\n",
    "                start = 0\n",
    "                label_dict = {}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "                        \n",
    "output_path = \"data/ner_dataset.json\"\n",
    "convert(output_path, 'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B-gpe', {'start': 20, 'end': 25, 'text': 'London'}, {'start': 29, 'end': 34, 'text': 'London'}], ['I-gpe', {'start': 36, 'end': 39, 'text': 'Rome'}]]\n"
     ]
    }
   ],
   "source": [
    "label_dict = { \"B-gpe\" : [{\"start\" : 20, \"end\" : 25, \"text\" : \"London\"}, {\"start\" : 29, \"end\" : 34, \"text\" : \"London\"}],\n",
    "                   \"I-gpe\" : [{\"start\" : 36, \"end\" : 39, \"text\" : \"Rome\"}]}\n",
    "label_list = []\n",
    "for ents in list(label_dict.keys()):\n",
    "    for i in range(len(label_dict[ents])):\n",
    "        if label_dict[ents][i]['text'] != '':\n",
    "            l = [ents, label_dict[ents][i]]\n",
    "            for j in range(i+1, len(label_dict[ents])):\n",
    "                if label_dict[ents][i]['text'] == label_dict[ents][j]['text']:      \n",
    "                    dl = {}\n",
    "                    dl[\"start\"] = label_dict[ents][j]['start']\n",
    "                    dl[\"end\"] = label_dict[ents][j]['end']\n",
    "                    dl[\"text\"] = label_dict[ents][i]['text']\n",
    "                    l.append(dl)\n",
    "                    label_dict[ents][j]['text'] = ''\n",
    "            label_list.append(l)\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert(output_path, unknown_label):\n",
    "#     try:\n",
    "#         fp = open(output_path, 'w')\n",
    "#         s = ''\n",
    "#         data_dict = {}\n",
    "#         annotation = []\n",
    "#         start = 0\n",
    "#         label_dict = {}\n",
    "#         for index, line in df.iterrows():\n",
    "#             word = line[0]\n",
    "#             entity = line[1]\n",
    "#             if word != '.':\n",
    "#                 s += word + ' '\n",
    "#                 if entity != 'O':\n",
    "#                     d = {}\n",
    "#                     d['start'] = start\n",
    "#                     d['end'] = start + len(word) - 1\n",
    "#                     d['text'] = word\n",
    "\n",
    "#                     try:\n",
    "#                         label_dict[entity].append(d)\n",
    "#                     except:\n",
    "#                         label_dict[entity] = []\n",
    "#                         label_dict[entity].append(d)\n",
    "#                 start += len(word) + 1\n",
    "#     '''label_dict = { \"B-gpe\" : [{\"start\" : 20, \"end\" : 25, \"text\" = \"London\"}, {\"start\" : 29, \"end\" : 33, \"text\" = \"India\"}],\n",
    "#                    \"I-gpe\" : [{\"start\" : 36, \"end\" : 39, \"text\" = \"Rome\"}]}'''   # label dict output will look like.\n",
    "#             else:\n",
    "#                 data_dict[\"content\"] = s\n",
    "#                 s = ''\n",
    "#                 label_list = []\n",
    "#                 for ents in list(label_dict.keys()):\n",
    "#                     for i in range(len(label_dict[ents])):\n",
    "#                         if label_dict[ents][i]['text'] != '':\n",
    "#                             l = [ents, label_dict[ents][i]]\n",
    "#                             for j in range(i+1 , len(label_dict[ents])):\n",
    "#                                 if label_dict[ents][i]['text'] == label_dict[ents][j]['text']:\n",
    "#                                     dl = {}\n",
    "#                                     dl[\"start\"] = label_dict[ents][j]['start']\n",
    "#                                     dl[\"end\"] = label_dict[ents][j]['end']\n",
    "#                                     dl[\"text\"] = label_dict[ents][j]['text']\n",
    "#                                     l.append(dl)\n",
    "#                                     label_dict[ents][j]['text'] = ''\n",
    "#                             label_list.append(l)\n",
    "#                 for entities in label_list:\n",
    "#                     label = entities\n",
    "#                     label['label'] = [entities[0]]\n",
    "#                     label['points'] = entities[1:]\n",
    "#                     annotations.append(label)\n",
    "#                 data_dict['annotation'] = annotations\n",
    "#                 annotations = []\n",
    "#                 json.dump(data_dict, fp)\n",
    "#                 fp.write('\\n')\n",
    "#                 data_dict = {}\n",
    "#                 start = 0\n",
    "#                 label_dict = {}\n",
    "#     except Exception as e:\n",
    "#         logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "#         return None\n",
    "                \n",
    "\n",
    "# output_path = \"data/ner_dataset.json\"\n",
    "# convert(output_path, 'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code of blog\n",
    "# # Convert json file to spaCy format.\n",
    "\n",
    "# @plac.annotations(input_file=(\"ner_dataset1.json\", \"option\", \"i\", str), output_file=(\"training_data1.pkl\", \"option\", \"o\", str))\n",
    "\n",
    "# def main(input_file=None, output_file=None):\n",
    "#     try:\n",
    "#         training_data = []\n",
    "#         lines=[]\n",
    "#         with open(input_file, 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "\n",
    "#         for line in lines:\n",
    "#             data = json.loads(line)\n",
    "#             text = data['content']\n",
    "#             entities = []\n",
    "#             for annotation in data['annotation']:\n",
    "#                 point = annotation['points'][0]\n",
    "#                 labels = annotation['label']\n",
    "#                 if not isinstance(labels, list):\n",
    "#                     labels = [labels]\n",
    "\n",
    "#                 for label in labels:\n",
    "#                     entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "#             training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "#         print(training_data)\n",
    "\n",
    "#         with open(output_file, 'wb') as fp:\n",
    "#             pickle.dump(training_data, fp)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.exception(\"Unable to process \" + input_file + \"\\n\" + \"error = \" + str(e))\n",
    "#         return None\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     plac.call(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_training_data(input_file = None, output_file = None):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines = []\n",
    "        with open(input_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for index, line in enumerate(lines):\n",
    "            line = json.loads(line)\n",
    "            content = line[\"content\"]\n",
    "            d = {}\n",
    "            entity_list = []\n",
    "            for ents in line[\"annotation\"]:\n",
    "                point = ents['points'][0]\n",
    "                labels = ents[\"label\"]\n",
    "\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    entity_list.append((point['start'], point['end'] + 1 , label))\n",
    "\n",
    "                d[\"entities\"] = entity_list\n",
    "            if d.get('entities') is not None:\n",
    "                training_data.append((content, d))\n",
    "\n",
    "        with open(output_file, 'wb') as fp:\n",
    "                pickle.dump(training_data, fp)\n",
    "\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + input_file + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "    \n",
    "training_data = extract_training_data(\"data/ner_dataset.json\", \"data/training_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # New entity labels\n",
    "# # Specify the new entity labels which you want to add here\n",
    "# LABEL = ['I-geo', 'B-geo', 'I-art', 'B-art', 'B-tim', 'B-nat', 'B-eve', 'O', 'I-per', 'I-tim', 'I-nat', 'I-eve', 'B-per', 'I-org', 'B-gpe', 'B-org', 'I-gpe']\n",
    "\n",
    "# # Loading training data \n",
    "# with open ('data/training_data.pkl', 'rb') as fp:\n",
    "#     TRAIN_DATA = pickle.load(fp)\n",
    "\n",
    "# def train(model = None, new_model_name = 'new_model', output_dir = None, n_iter = 10):\n",
    "#     \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
    "#     if model is not None:\n",
    "#         nlp = spacy.load(model)  # load existing spacy model\n",
    "#         print(\"Loaded model '%s'\" % model)\n",
    "#     else:\n",
    "#         nlp = spacy.blank('en')  # create blank Language class\n",
    "#         print(\"Created blank 'en' model\")\n",
    "#     if 'ner' not in nlp.pipe_names:\n",
    "#         ner = nlp.create_pipe('ner')\n",
    "#         nlp.add_pipe(ner)\n",
    "#     else:\n",
    "#         ner = nlp.get_pipe('ner')\n",
    "\n",
    "#     for i in LABEL:\n",
    "#         ner.add_label(i)   # Add new entity labels to entity recognizer\n",
    "\n",
    "#     if model is None:\n",
    "#         optimizer = nlp.begin_training()\n",
    "#     else:\n",
    "#         optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "#     # Get names of other pipes to disable them during training to train only NER\n",
    "#     other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "#     with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "#         for itn in tqdm(range(n_iter)):\n",
    "#             random.shuffle(TRAIN_DATA)\n",
    "#             losses = {}\n",
    "#             batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "#             for batch in batches:\n",
    "#                 texts, annotations = zip(*batch)\n",
    "#                 nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "#                            losses=losses)\n",
    "#             print('Losses', losses)\n",
    "\n",
    "#     # Test the trained model\n",
    "#     test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "#     doc = nlp(test_text)\n",
    "#     print(\"Entities in '%s'\" % test_text)\n",
    "#     for ent in doc.ents:\n",
    "#         print(ent.label_, ent.text)\n",
    "\n",
    "#     # Save model \n",
    "#     if output_dir is not None:\n",
    "#         output_dir = Path(output_dir)\n",
    "#         if not output_dir.exists():\n",
    "#             output_dir.mkdir()\n",
    "#         nlp.meta['name'] = new_model_name  # rename model\n",
    "#         nlp.to_disk(output_dir)\n",
    "#         print(\"Saved model to\", output_dir)\n",
    "\n",
    "#         # Test the saved model\n",
    "#         print(\"Loading from\", output_dir)\n",
    "#         nlp2 = spacy.load(output_dir)\n",
    "#         doc2 = nlp2(test_text)\n",
    "#         for ent in doc2.ents:\n",
    "#             print(ent.label_, ent.text)\n",
    "\n",
    "# train(model = None, new_model_name = 'st_ner', output_dir = 'model/customNERmodel', n_iter = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_path():\n",
    "    return 'models/customNERmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_dir, nlp, new_model_name):\n",
    "    output_dir = f'../Custom NER using spacy/{output_dir}'\n",
    "    if output_dir is not None:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        nlp.meta['name'] = new_model_name\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to \", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EntityMatcher(object):\n",
    "#     name = \"entity_matcher\"\n",
    "\n",
    "#     def __init__(self, nlp, terms, label):\n",
    "#         patterns = [nlp.make_doc(text) for text in terms]\n",
    "#         self.matcher = PhraseMatcher(nlp.vocab)\n",
    "#         self.matcher.add(label, None, *patterns)\n",
    "\n",
    "#     def __call__(self, doc):\n",
    "#         matches = self.matcher(doc)\n",
    "#         seen_tokens = set()\n",
    "#         new_entities = []\n",
    "#         entities = doc.ents\n",
    "#         for match_id, start, end in matches:\n",
    "#         #    span = Span(doc, start, end, label=match_id)\n",
    "#         #    doc.ents = list(doc.ents) + [span]\n",
    "#             # check for end - 1 here because boundaries are inclusive\n",
    "#             if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "#                 new_entities.append(Span(doc, start, end, label=match_id))\n",
    "#                 entities = [\n",
    "#                     e for e in entities if not (e.start < end and e.end > start)\n",
    "#                 ]\n",
    "#                 seen_tokens.update(range(start, end))\n",
    "\n",
    "#         doc.ents = tuple(entities) + tuple(new_entities)\n",
    "#         return doc\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# terms = (u\"cat\", u\"dog\", u\"tree kangaroo\", u\"giant sea spider\", u\"Barack Obama\")\n",
    "# #terms = ('Barack Obama',)\n",
    "# entity_matcher = EntityMatcher(nlp, terms, \"PRESIDENT\")\n",
    "\n",
    "# nlp.add_pipe(entity_matcher, after=\"ner\")\n",
    "\n",
    "# print(nlp.pipe_names)  # The components in the pipeline\n",
    "\n",
    "# doc = nlp(u\"This is a text about Barack Obama and a tree kangaroo\")\n",
    "# print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_added = []\n",
    "# for _, annotation in training_data:\n",
    "#     if annotation.get('entities') is not None:\n",
    "#         for ent in annotation.get('entities'):\n",
    "#             if ent[2] not in label_added:\n",
    "#                 label_added.append(ent[2])\n",
    "# print(label_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for _, annotation in training_data:\n",
    "    if annotation.get('entities') is None:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for _, annotation in training_data:\n",
    "#     if annotation.get('entities') is None:\n",
    "#         data.append(_, annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = ['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\\\n",
    "       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\\\n",
    "       'I-eve', 'I-nat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, model_path, model = None, n_iter = 20):\n",
    "    # Load the model\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model_path)\n",
    "        print(\"Loading model '%s' \", model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')\n",
    "        print(\"Blank Model Created\")\n",
    "    \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "        \n",
    "    # Adding new entry\n",
    "    # This will not take into account 'O' and time consuming also so directly we make a list of labels and add them in pipeline.\n",
    "#     label_added = []\n",
    "#     for _, annotation in training_data:\n",
    "#         if annotation.get('entities') is not None:\n",
    "#             for ent in annotation.get('entities'):\n",
    "#                 if ent[2] not in label_added:\n",
    "#                     label_added.append(ent[2])\n",
    "#                     ner.add_label(ent[2])\n",
    "    \n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)\n",
    "    \n",
    "    # Initialising optimiser\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "        \n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "    other_pipes = [other_pipe for other_pipe in nlp.pipe_names if other_pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        for itn in tqdm(range(n_iter)):\n",
    "            random.shuffle(training_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(training_data, size = compounding(4.0, 32.0, 1.001))\n",
    "            \n",
    "            for batch in batches:\n",
    "                text, annotation = zip(*batch)\n",
    "                nlp.update(text, annotation, sgd = optimizer, drop = 0.35, losses = losses)\n",
    "        print('Losses', losses)\n",
    "        save_model(model_path, nlp, 'st_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank Model Created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [16:15<00:00, 487.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 65237.121864034256}\n",
      "Saved model to  ../Custom NER using spacy/models/customNERmodel\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_pickle(\"data/training_data.pkl\")\n",
    "model_path = get_model_out_path()\n",
    "\n",
    "train(training_data, model_path, model = None, n_iter = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODELS_BASE_PATH = '../Custom NER using spacy/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from  ../Custom NER using spacy/models/\n",
      "Entities in 'Gianni Infantino is the president of FIFA.'\n",
      "B-geo Gianni\n",
      "I-per Infantino\n",
      "B-org FIFA\n"
     ]
    }
   ],
   "source": [
    "def predict_entities(text, model):\n",
    "    doc = model(text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    ent_array = []\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "if TRAINED_MODELS_BASE_PATH is not None:\n",
    "    print(\"Loading models from \", TRAINED_MODELS_BASE_PATH)\n",
    "    model = spacy.load(TRAINED_MODELS_BASE_PATH + 'customNERmodel')\n",
    "    predict_entities(test_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'Gianni Infantino is the president of FIFA.'\n",
      "B-per John\n",
      "I-per Lee\n",
      "B-org CBSE\n"
     ]
    }
   ],
   "source": [
    "text = 'John Lee is the chief of CBSE'\n",
    "predict_entities(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'Gianni Infantino is the president of FIFA.'\n",
      "B-gpe Americans\n"
     ]
    }
   ],
   "source": [
    "text = 'Americans suffered from H5N1'\n",
    "predict_entities(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
